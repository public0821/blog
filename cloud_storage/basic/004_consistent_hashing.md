# 一致性hashing在云存储中的应用概述

在分布式系统中，有很多个节点，在处理请求的时候，一般是将请求进行hash，然后根据节点的数量进行取模，得到请求最终发往的节点上，这样可以让每个节点的负载变得均衡，但问题是，如果节点的数量发生变化，hash后再取模，很多request就会发到其它的节点上去，对于无状态的应用来说，问题不大，但对于有状态的应用来说，就会带来很大的问题，比如cache，如果大量的request发到其它的节点上去，就会造成cache失效，需要cache节点重启加载数据，对于存储来说，更严重，原来存储到A节点的数据，现在变成了存在B节点上，就涉及到数据的迁移，否则新的request发到B节点后，会找不到数据。

一致性hashing要解决的问题就是当节点数量发生变化的时候，只让少量的数据迁移到其它的node，并且保持节点负载的均衡。通常采用如下这些方法。

## 传统hashing

假设我们节点的数量是5，对于传统的hashing，每个节点对应一个bucket（slot），相当于hash表的长度是5，对于10~14这5个数字而言，在hash表中的位置如下
```
   0   1    2    3    4    
+----+----+----+----+----+
| 10 | 11 | 12 | 13 | 14 |    
+----+----+----+----+----+
```

* 增加一个节点会变成下面的样子
```
   0   1    2    3    4    5
+----+----+----+----+----+----+
| 12 | 13 | 14 |    | 10 | 11 |
+----+----+----+----+----+----+
```

* 减少一个节点后变成下面的样子
```
   0   1      2     3    
+----+----+-------+----+
| 12 | 13 | 10,14 | 11 |
+----+----+-------+----+
```

从上面可以看出，当增加和减少一个节点的时候，这些数字在hash表中的位置全部都发生了变化

## 改进1 （解决移动多的问题）
上面传统hash的问题主要是hash表的长度是节点的数量，现在考虑一种改进办法，hash表的长度固定，每个节点对应一段范围的hash值，这个固定长度可以很长，比如int的范围，这里为了简单起见，假设hash表的长度是10，节点数是5

这里假设5个节点均匀的分布在hash表中，还是上面几个数字，会得到下面的结果，10和11在n1上，12和13在n2上，14在n3上，n4和n5上没有数据
```
   0   1    2    3    4    5    6    7    8     9
+----+----+----+----+----+----+----+----+----+----+
| 10 | 11 | 12 | 13 | 14 |    |    |    |    |    |
+----+----+----+----+----+----+----+----+----+----+
          ↓         ↓         ↓         ↓         ↓
          n1        n2        n3        n4        n5
```

* 增加一个节点，假设把增加的节点放在n1和n2之间，可以看出只有12的位置发生了变化，从n2变到了n6，其它的都没变
```
   0   1    2    3    4    5    6    7    8     9
+----+----+----+----+----+----+----+----+----+----+
| 10 | 11 | 12 | 13 | 14 |    |    |    |    |    |
+----+----+----+----+----+----+----+----+----+----+
          ↓    ↓    ↓         ↓         ↓         ↓
          n1   n6   n2        n3        n4        n5
```

* 删除一个节点，假设删掉n2，可以看出只有12和13的位置发生了变化，从n2变到了n3
```
   0   1    2    3    4    5    6    7    8     9
+----+----+----+----+----+----+----+----+----+----+
| 10 | 11 | 12 | 13 | 14 |    |    |    |    |    |
+----+----+----+----+----+----+----+----+----+----+
          ↓                   ↓         ↓         ↓
          n1                  n3        n4        n5
```

从上面可看出，这种方法当节点数量发生变化的时候，只有很少一部分数据所属的节点发生变化，但这种方法有很多缺点。
  * 数据hash并得到它在hash表的位置后，还要根据范围查找到它所属于的节点，查找的过程需要O(logn)的复杂度
  * hash表的长度固定，但节点数量在变化，如何根据节点数来分配每个节点的范围？
    * 一种是这里例子中的平均分配，但很明显，当增加和减少节点后，节点的负载会很不均衡
    * 一种是随机(或者hash节点并映射到hash表中)，当节点数量较少时，随机也不能保证均衡（随机一般只有在数据基数比较大时才会相对比较平均）
    
## 改进2 （解决分配不均衡问题）
一种解决办法是增加虚拟节点，将一个物理节点变成N个虚拟节点，虚拟节点到物理节点的对应关系固定。

但这种方法也不能采取平均分配的方法，因为当物理节点的数量发生变化时，虚拟节点的数量也发生了变化，由于hash表的长度不会变，所以还是有不均衡的问题。

对于随机的办法，由于虚拟节点变多了，随机之后会相对比较均衡，出现大的不均衡的几率会大大降低，但这种方法还是有缺点。
  * 不可控，适用于对于均衡性要求不是很高的场合，
  * 没有解决查找时需要O(logn)时间复杂度的问题。
  * 对于云存储来说，当节点发生变化时，虚拟节点所负责的范围可能会发生变化，在实现的过程中，如何只将变化的那部分移动到其它节点对某些架构来说是一个挑战。

>到这里为止，就是我们常说的一致性hashing。

## 改进3 （解决分配不均衡问题和查找慢的问题）
还是添加虚拟节点，但这次虚拟节点跟物理节点的关系不固定，并且虚拟节点的数量固定。

由于虚拟节点的数量和hash表的长度都是固定的，所以每个虚拟节点负责的范围也是固定不变的，这样的一个好处是当虚拟节点对应的物理节点发生变化的时候，可以将整个虚拟节点的所有数据移动到新节点上去，在代码实现上来说会容易很多。

但这也引入了一个问题，虚拟节点的数量设置多少合适？这个数量跟hash表的长度不一样，hash表的长度只是一个虚拟的范围，不对应具体的实体，所以长一点没关系。
  * 虚拟节点太少
    * 会导致一个物理节点对应的虚拟节点太少，从而一个虚拟节点上存放的数据会很多，不利于增加/减少物理节点时数据的迁移，也影响磁盘坏掉之后的数据恢复速度，因为一个虚拟节点只能移动到一个物理节点上，一个物理节点拥有的虚拟节点越多，移动的时候并发性就越好，
    * 当物理节点增长时，可能会出现虚拟节点不够用的情况（某些物理节点没有对应的虚拟节点）
  * 虚拟节点设置的太多
    * 保存虚拟节点到物理节点的对应关系要消耗很多空间，不利于在网络共享
    * 在实际代码实现的时候，每个虚拟节点的维护都需要一定成本，虚拟节点越多，需要的系统资源会越多

由于虚拟节点是固定的，所以根据hash值能O(1)复杂度内找到虚拟节点，那么虚拟节点到物理节点的对应关系怎么管理呢？这个要看具体的实现，但大概思路是一样的。
1. 先算出每个物理节点平均应该拥有的虚拟节点个数，然后随机的或者按照顺序的给它们建立映射关系，比如有4个物理几点，100个虚拟节点，就可以简单的前25个虚拟节点给第一个物理节点，然后后面的以此类推，只要数据hash后是分散的，每个物理节点收到的请求量就是平均的
2. 当增加节点时，算出增加节点后每个物理节点平均拥有的虚拟节点个数，对大于这个数量的节点，让出对应数量的虚拟节点给新节点
3. 当减少节点时，算出减少节点后每个物理节点平均拥有的虚拟节点个数，对小于这个数量的节点，接收相应数量的虚拟节点

维护映射关系需要多少内存？假设我们有65536个虚拟节点，由于物理节点的数量不会超过这个数量，我们只需要一个存放两字节整数的数组就可以了，index就是虚拟节点ID，里面的数据就是物理节点ID，那么理论上需要65536*2大于12M内存。

## 如何映射多份copy
前面介绍的都是一个数据映射到一个物理节点，对于云存储来说，经常需要一个数据保存多份来保持数据的持久性，这时候一份数据就需要对应多个物理节点，这种又是怎么映射的呢？

这里以Openstack Swift为例，介绍是如何管理虚拟节点到物理节点的映射关系。


## 参考

* [Consistent Hashing](http://courses.cse.tamu.edu/caverlee/csce438/readings/consistent-hashing.pdf)
* [Building a Consistent Hashing Ring](https://docs.openstack.org/swift/latest/ring_background.html)
